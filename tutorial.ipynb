{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Mastering LLM Fine-Tuning with TRL\n",
    "\n",
    "Welcome! In this session, you'll learn how to fine-tune large language models (LLMs) using ğŸ¤— TRL.\n",
    "We'll cover key concepts, install necessary tools, and prepare for diving into Supervised Fine-Tuning (SFT) and Group-Relative Policy Optimization (GRPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤” Do you remember how LLMs work?\n",
    "\n",
    "LLMs are essentially highly advanced autocomplete systems.\n",
    "You provide them with a bit of text, and they predict what comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(task=\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\")\n",
    "prompt = \"Octopuses have three\"\n",
    "pipeline(prompt, max_new_tokens=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, octopuses have three hearts ğŸ«€! I didn't know before I wrote this notebook to be honnest.\n",
    "\n",
    "The problem with `pipeline` is that it hides the underlying details that are essential to understand before proceeding. Let's break down the pipeline to examine what is happening behind the scenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸª™ Tokenization\n",
    "\n",
    "The first step is making sure the model can understand the text. This is done by transforming the text into tokens. Tokens are small units of text that the model can interpret. \n",
    "The tokenizer is responsible for encoding the text into token ids and decoding the token ids back into text.\n",
    "Let's use the same example as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "prompt = \"Octopuses have three\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the token mapping:\n",
    "\n",
    "| Text   | `Oct`  | `op` | `uses` | `â£have` | `â£three` |\n",
    "|--------|--------|------|--------|---------|----------|\n",
    "| Tokens | `18053`| `453`|  `4776`|    `614`|    `2326`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â© The Forward Pass\n",
    "\n",
    "Now that we have a list of integers, we can pass them to the model. Let's see what happens when we do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B\").to(\"cuda\")\n",
    "output = model(**inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I seems not to output text ğŸ¥º. Let's see what it returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a `CausalLMOutputWithPast` object, which contains several attributes. The only one we are concerned with for now is `logits`. To understand what this is, let's first check its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits tensor is 3-dimensional:\n",
    "- `batch_size`: 1 (since we only have one sequence)\n",
    "- `sequence_length`: 5 (because the text was tokenized into 5 tokens, as seen earlier)\n",
    "- `vocab_size`: 151936 (the total number of unique tokens the tokenizer can map to an integer).\n",
    "\n",
    "**But what are these logits?**\n",
    "\n",
    "Logits are the scores that the model assigns to each token in the vocabulary for the next position in the sequence. They represent the model's confidence in predicting each token as the next one in the sequence.\n",
    "Conceptually, you can think of logits as a probability distribution over the vocabulary. The model is saying, \"Given the input sequence, here are my scores for each possible next token.\"\n",
    "\n",
    "Consequently, the last column of the logits tensor corresponds to the model's prediction for the next token in the sequence. In other word, what comes after `\"Octopuses have three\"`? To get a better unserstanding, let's try to plot the distribution of the logits for the last token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Take only the logits of the last token\n",
    "last_logits = output.logits[0, -1, :]  # shape = (151936,)\n",
    "last_probs = torch.softmax(last_logits, dim=-1)  # turn logits into probabilities\n",
    "\n",
    "# Let's consider only the 10 most probable tokens\n",
    "top_last_probs, top_last_ids = torch.topk(last_probs, k=10)\n",
    "\n",
    "print(f\"The most probable next token ids are: {top_last_ids.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If models understand tokens ids, I don't. So let's decode these ids and see what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_last_tokens = tokenizer.batch_decode(top_last_ids)\n",
    "print(f\"The most probable next tokens are: {top_last_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distribution to better understand what the model is predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(top_last_tokens, top_last_probs.tolist())\n",
    "\n",
    "plt.xlabel('Next token')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Logit')\n",
    "plt.title('Octopuses have three...')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done it for the last token in the sequence, but we can do it for all tokens in the sequence. Let's see what the model is predicting for each token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.softmax(output.logits[0], dim=-1)  # turn logits into probabilities, shape = (5, 151936)\n",
    "# Let's consider only the 10 most probable tokens\n",
    "top_probs, top_ids = torch.topk(probs, k=10)\n",
    "top_tokens = [tokenizer.batch_decode(ids) for ids in top_ids]\n",
    "# Replace \" \" with \"â£\" for better visualization\n",
    "top_tokens = [[token.replace(\" \", \"â£\") for token in tokens] for tokens in top_tokens]\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(16, 3))\n",
    "for i in range(5):\n",
    "    ax[i].bar(tokenizer.batch_decode(top_ids[i]), top_probs[i].tolist())\n",
    "    ax[i].set_xticks(range(10))\n",
    "    ax[i].set_xticklabels(top_tokens[i], rotation=45)\n",
    "    ax[i].set_xlabel('Next token')\n",
    "    ax[i].set_ylabel('Probability')\n",
    "    partial_seq = tokenizer.decode(inputs['input_ids'][0][:i + 1])\n",
    "    ax[i].set_title(f\"{partial_seq}...\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty interesting, right? At each step, we can see what the model thinks as the most likely next token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/tuto_forward_pass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” How do we end up with a model capable of outptting such distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs donâ€™t start out smart â€” far from it. The impressive ability to generate coherent, relevant text comes from a process called **pretraining**.\n",
    "\n",
    "#### ğŸ—ï¸ What is Pretraining?\n",
    "\n",
    "During pretraining, a model starts with **random weights** and learns by trying to **predict the next token** in massive amounts of text â€” often hundreds of billions of tokens scraped from the internet. This process teaches the model general language patterns, grammar, and world knowledge.\n",
    "\n",
    "Pretraining is:\n",
    "\n",
    "* **Massive** in scale (weeks or months on hundreds of GPUs)\n",
    "* **Costly** (millions of dollars)\n",
    "* **Foundational** â€” it's what makes an LLM even remotely useful\n",
    "\n",
    "Once this phase is complete, we get whatâ€™s called a **base model**.\n",
    "\n",
    "#### ğŸ§ª What is a Base Model?\n",
    "\n",
    "A base model is pretrained, but it hasn't been taught *how* to behave.\n",
    "\n",
    "It doesn't follow instructions well.  \n",
    "It doesnâ€™t know how to have a conversation or answer questions directly.  \n",
    "It simply continues text based on patterns it has seen.\n",
    "\n",
    "So if you give it a prompt like:\n",
    "\n",
    "> *â€œWhat is the capital of Germany?â€*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the capital of Germany?\\n\"\n",
    "print(pipeline(prompt, max_new_tokens=100)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generates something that *looks* like a multiple-choice question â€” because it has seen many of those during pretraining â€” but it wonâ€™t actually answer the question.\n",
    "Why? Because it hasnâ€™t been trained to respond helpfully. No one has told it: *\"This is how you should respond.\"*\n",
    "\n",
    "#### ğŸ§  What About ChatGPT, Claude, and Others?\n",
    "\n",
    "When you use popular models like GPT-4o, Claude, DeepSeek-R1, or o3, youâ€™re *not* using a base model.\n",
    "\n",
    "Youâ€™re using a model thatâ€™s been **fine-tuned** â€” and often **reinforcement-aligned** â€” to be helpful, safe, and responsive.\n",
    "\n",
    "For example:\n",
    "\n",
    "- **DeepSeek-R1** is fine-tuned from a model called **DeepSeek-V3-Base**.  \n",
    "- **OpenAI o4-mini** is a fine-tuned version of an unknown base model.  \n",
    "- **Llama 4 Scout** (officially: *Llama-4-Scout-17B-16E-Instruct*) is a fine-tuned version of *Llama-4-Scout-17B-16E*.\n",
    "\n",
    "### ğŸ¯ Why Does This Matter?\n",
    "\n",
    "In this tutorial, weâ€™re starting with a **base model** â€” one that can generate text, but isnâ€™t yet useful on its own.\n",
    "\n",
    "It wonâ€™t follow instructions well, and it may not be helpful or safe by default.\n",
    "\n",
    "Your job is to **fine-tune** it into something smarter, more helpful, or more aligned to your specific goals.\n",
    "\n",
    "Thatâ€™s the magic of **post-training** â€” and thatâ€™s where **TRL** and your creativity come in.\n",
    "\n",
    "![image.png](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/tuto_pretraining_posttraining.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â˜• At this point, I think it's a good time to take a break. Let's grab a coffee and come back in 10 minutes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ Fine-tuning\n",
    "\n",
    "In the previous section, we discussed what pretraining is and how it gives us a base model. To recap, a base model is one that has been pretrained on a huge dataset, but hasnâ€™t yet been adapted for specific tasks. It can generate text, but by itself it isnâ€™t particularly useful.\n",
    "\n",
    "### ğŸ—£ï¸ Chat template\n",
    "\n",
    "For instance, it canâ€™t function as a chatbot out of the box. Letâ€™s try it and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "How many hearts do octopuses have?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "print(pipeline(prompt, max_new_tokens=20)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I donâ€™t even know how to describe that output. But whatâ€™s clear is that itâ€™s not satisfactory.\n",
    "\n",
    "You may have noticed that I used a special format to represent the conversation. Specifically, I used custom tokens like `<|im_start|>` and `<|im_end|>`. This kind of formatting is very convenient and easy to parse for later use. It's known as a *chat template*. Tokenizers are also capable of handling these templatesâ€”as long as you specify the one you want using Jinja2 format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "tokenizer.chat_template = \"\"\"{{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n",
    "{%- for message in messages %}\n",
    "    {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
    "{%- endfor %}\n",
    "{%- if add_generation_prompt %}\n",
    "    {{- '<|im_start|>assistant\\n' }}\n",
    "{%- endif %}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use it to format conversations properly. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How many hearts do octopuses have?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Octopuses have three hearts.\"},\n",
    "]\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesnâ€™t solve our problem yetâ€”but at least now we have a tool to format conversations properly.\n",
    "\n",
    "### ğŸ—‚ï¸ Conversational Data\n",
    "\n",
    "Letâ€™s recap. At this point, we have a model that can generate text, but itâ€™s not yet capable of holding a conversation. We also have a tool to format dialogues using a chat template. So, whatâ€™s still missing?\n",
    "\n",
    "You guessed itâ€”just look at the section title. What weâ€™re missing is *data*.\n",
    "\n",
    "Hugging Face tools have already been incredibly helpful, even if you didnâ€™t notice. First, you loaded the model using the `transformers` library. Then, you loaded the tokenizer the same way. Both the model and tokenizer were automatically downloaded for you from the ğŸ¤— Hugging Face Hub.\n",
    "\n",
    "Hereâ€™s the great part: the Hub doesnâ€™t just host modelsâ€”it also offers a huge collection of datasets. As of writing, there are 381,735! Letâ€™s go check one out.\n",
    "\n",
    "Letâ€™s pick a conversational dataset, like [Open-Thoughts-114k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k). To load it, weâ€™ll use another fantastic library in the Hugging Face ecosystem: `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"open-thoughts/OpenThoughts-114k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s take a peek at what this dataset containsâ€”starting with the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw output might not be very readable, but you can always explore it visually on the Hugging Face Hub. Whatâ€™s most important for us is that this dataset contains conversationsâ€”in the `conversations` column. So letâ€™s try formatting one of them using our chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.apply_chat_template(example[\"conversations\"], tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh! `UndefinedError: 'dict object' has no attribute 'role'`. Looks like the dataset isnâ€™t in the format we expected. Yep, that happensâ€”and itâ€™s actually pretty common.\n",
    "\n",
    "Whenever you're training models, youâ€™ll almost always have to go through a data preprocessing step. So letâ€™s tackle that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§¹ Data Preparation\n",
    "\n",
    "What we want is for each conversation to look like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"How many hearts do octopuses have?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Octopuses have three hearts.\"},\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "But the dataset actually looks like this:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"conversations\": [\n",
    "        {\"from\": \"human\", \"value\": \"How many hearts do octopuses have?\"},\n",
    "        {\"from\": \"assistant\", \"value\": \"Octopuses have three hearts.\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Letâ€™s write a function to convert from the second format to the one we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    messages = []\n",
    "    for message in example[\"conversations\"]:\n",
    "        role = message[\"from\"]\n",
    "        content = message[\"value\"]\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        messages.append(message)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "format_example(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now, how do we apply this function to the entire dataset? Simpleâ€”just use `dataset.map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(format_example, remove_columns=[\"conversations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quick. Letâ€™s now try formatting the first example using our chat template again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "print(tokenizer.apply_chat_template(example[\"messages\"], tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VoilÃ ! We now have a dataset of properly formatted conversations. It's ready to be used for training our model.\n",
    "\n",
    "\n",
    "> Waita second! We forgot to apply the chat template to the entire dataset!\n",
    "\n",
    "No worries, the trainer will take care of that for us. ğŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ•µï¸ Supervised Fine-Tuning (SFT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
